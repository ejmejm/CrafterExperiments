{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "from gym import Wrapper\n",
    "import crafter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AchievementInfoWrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        info['achievement_count'] = np.sum(np.array(list(\n",
    "            info['achievements'].values())) > 0)\n",
    "        info['achievement_frac'] = info['achievement_count'] / len(info['achievements'])\n",
    "        for key in info ['achievements']:\n",
    "            info[key] = info['achievements'][key]\n",
    "        return observation, reward, done, info\n",
    "\n",
    "LOG_DIR = 'tmp/'\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def make_env(env_name='CrafterReward-v1', data_dir='data/', n_envs=8):\n",
    "    env = gym.make(env_name)\n",
    "    env = AchievementInfoWrapper(env)\n",
    "    # env = crafter.Recorder(\n",
    "    #     env, data_dir,\n",
    "    #     save_stats=True,\n",
    "    #     save_episode=False,\n",
    "    #     save_video=False,\n",
    "    # )\n",
    "    env.reset()\n",
    "    info_sample = env.step(0)[3]\n",
    "    env = SubprocVecEnv([lambda: env for i in range(n_envs)])\n",
    "    env = VecMonitor(env, LOG_DIR, info_keywords=(\n",
    "        'achievement_count', 'achievement_frac') + \\\n",
    "        tuple(info_sample['achievements'].keys()))\n",
    "    return env\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, log_dir=LOG_DIR, verbose=0):\n",
    "        self.log_dir = log_dir\n",
    "        self.df_idx = 0\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        df = load_results(self.log_dir)\n",
    "        n_rows = df.shape[0]\n",
    "        df = df.iloc[self.df_idx:n_rows]\n",
    "        df.drop(columns=['index', 'r', 't', 'l'], inplace=True)\n",
    "        for key in df.columns:\n",
    "            if 'achievement' in key:\n",
    "                val = np.mean(df[key].values)\n",
    "            else:\n",
    "                val = np.mean(df[key].values > 0)\n",
    "            self.logger.record('achievement/' + key, val)\n",
    "        self.df_idx = n_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CrafterReward-v1'\n",
    "n_steps = int(2e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#   env = make_env(env_name)\n",
    "# obs = env.reset()\n",
    "# obs, reward, done, info = env.step(env.action_space.sample())\n",
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/PPO_3\n",
      "-------------------------------------\n",
      "| achievement/          |           |\n",
      "|    achievement_count  | 2.43      |\n",
      "|    achievement_frac   | 0.111     |\n",
      "|    collect_coal       | 0         |\n",
      "|    collect_diamond    | 0         |\n",
      "|    collect_drink      | 0.261     |\n",
      "|    collect_iron       | 0         |\n",
      "|    collect_sapling    | 0.565     |\n",
      "|    collect_stone      | 0         |\n",
      "|    collect_wood       | 0.217     |\n",
      "|    defeat_skeleton    | 0         |\n",
      "|    defeat_zombie      | 0         |\n",
      "|    eat_cow            | 0         |\n",
      "|    eat_plant          | 0         |\n",
      "|    make_iron_pickaxe  | 0         |\n",
      "|    make_iron_sword    | 0         |\n",
      "|    make_stone_pickaxe | 0         |\n",
      "|    make_stone_sword   | 0         |\n",
      "|    make_wood_pickaxe  | 0         |\n",
      "|    make_wood_sword    | 0         |\n",
      "|    place_furnace      | 0         |\n",
      "|    place_plant        | 0.435     |\n",
      "|    place_stone        | 0         |\n",
      "|    place_table        | 0         |\n",
      "|    wake_up            | 0.957     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 177       |\n",
      "|    ep_rew_mean        | 1.5347824 |\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 1         |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 4096      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| achievement/            |           |\n",
      "|    achievement_count    | 3.14      |\n",
      "|    achievement_frac     | 0.143     |\n",
      "|    collect_coal         | 0         |\n",
      "|    collect_diamond      | 0         |\n",
      "|    collect_drink        | 0.318     |\n",
      "|    collect_iron         | 0         |\n",
      "|    collect_sapling      | 0.636     |\n",
      "|    collect_stone        | 0         |\n",
      "|    collect_wood         | 0.409     |\n",
      "|    defeat_skeleton      | 0         |\n",
      "|    defeat_zombie        | 0         |\n",
      "|    eat_cow              | 0         |\n",
      "|    eat_plant            | 0         |\n",
      "|    make_iron_pickaxe    | 0         |\n",
      "|    make_iron_sword      | 0         |\n",
      "|    make_stone_pickaxe   | 0         |\n",
      "|    make_stone_sword     | 0         |\n",
      "|    make_wood_pickaxe    | 0         |\n",
      "|    make_wood_sword      | 0         |\n",
      "|    place_furnace        | 0         |\n",
      "|    place_plant          | 0.636     |\n",
      "|    place_stone          | 0         |\n",
      "|    place_table          | 0.136     |\n",
      "|    wake_up              | 1         |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 179       |\n",
      "|    ep_rew_mean          | 1.8555554 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 209       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 39        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0159922 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.82     |\n",
      "|    explained_variance   | 0.0138    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.034    |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -0.028    |\n",
      "|    value_loss           | 0.0525    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| achievement/            |             |\n",
      "|    achievement_count    | 2.96        |\n",
      "|    achievement_frac     | 0.135       |\n",
      "|    collect_coal         | 0           |\n",
      "|    collect_diamond      | 0           |\n",
      "|    collect_drink        | 0.28        |\n",
      "|    collect_iron         | 0           |\n",
      "|    collect_sapling      | 0.68        |\n",
      "|    collect_stone        | 0           |\n",
      "|    collect_wood         | 0.28        |\n",
      "|    defeat_skeleton      | 0           |\n",
      "|    defeat_zombie        | 0           |\n",
      "|    eat_cow              | 0           |\n",
      "|    eat_plant            | 0           |\n",
      "|    make_iron_pickaxe    | 0           |\n",
      "|    make_iron_sword      | 0           |\n",
      "|    make_stone_pickaxe   | 0           |\n",
      "|    make_stone_sword     | 0           |\n",
      "|    make_wood_pickaxe    | 0           |\n",
      "|    make_wood_sword      | 0.04        |\n",
      "|    place_furnace        | 0           |\n",
      "|    place_plant          | 0.6         |\n",
      "|    place_stone          | 0           |\n",
      "|    place_table          | 0.12        |\n",
      "|    wake_up              | 0.96        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 173         |\n",
      "|    ep_rew_mean          | 1.9285715   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023232166 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.057       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log='./logs/')\n",
    "model.learn(total_timesteps=n_steps, callback=TensorboardCallback())\n",
    "model.save('models/ppo_baseline_' + env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the trained agent\n",
    "# # NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# # to compare the system on which the model was trained vs the current one\n",
    "# # model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "# model = DQN.load(\"dqn_lunar\", env=env)\n",
    "\n",
    "# # Evaluate the agent\n",
    "# # NOTE: If you use wrappers with your environment that modify rewards,\n",
    "# #       this will be reflected here. To evaluate with original rewards,\n",
    "# #       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "# mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# # Enjoy trained agent\n",
    "# obs = env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1fe9e103656f2266082d3c8033b60b2c460ce6c85d6ed8c56af72efc68894ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
